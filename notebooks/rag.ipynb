{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from typing import Iterable, cast\n",
    "\n",
    "import gel\n",
    "import gel.ai\n",
    "import tiktoken\n",
    "from pydantic import BaseModel, TypeAdapter\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.messages import (\n",
    "    FinalResultEvent,\n",
    "    FunctionToolCallEvent,\n",
    "    FunctionToolResultEvent,\n",
    "    PartDeltaEvent,\n",
    "    PartStartEvent,\n",
    "    TextPart,\n",
    "    TextPartDelta,\n",
    "    ToolCallPartDelta,\n",
    ")\n",
    "from settings import (\n",
    "    EMBEDDING_MODEL_MAX_TOKENS,\n",
    "    EMBEDDING_MODEL_NAME,\n",
    "    NANAPI_CLIENT_ID,\n",
    "    PYDANTIC_AI_DEFAULT_MODEL_NAME,\n",
    "    PYDANTIC_AI_MODEL_CLS,\n",
    "    PYDANTIC_AI_PROVIDER,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cast(\n",
    "    gel.AsyncIOClient,\n",
    "    gel.create_async_client().with_globals(client_id=NANAPI_CLIENT_ID),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(BaseModel):\n",
    "    id: str\n",
    "    username: str\n",
    "    global_name: str | None\n",
    "    bot: bool | None = None\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    id: str\n",
    "    channel_id: str\n",
    "    content: str\n",
    "    timestamp: datetime\n",
    "    author: Author\n",
    "\n",
    "\n",
    "class MessagePage(BaseModel):\n",
    "    context: str\n",
    "    channel_id: str\n",
    "    messages: list[Message] | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = await client.query(\n",
    "    r\"\"\"\n",
    "    select discord::Message { * }\n",
    "    filter not exists .deleted_at\n",
    "    and .timestamp > <datetime>\"2024-01-01T00:00:00+00:00\"\n",
    "    order by .timestamp asc\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_messages = [Message.model_validate_json(item.data) for item in resp1]\n",
    "all_messages = [m for m in all_messages if not m.author.bot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_channel_messages = defaultdict[str, list[Message]](list)\n",
    "for message in all_messages:\n",
    "    all_channel_messages[message.channel_id].append(message)\n",
    "# print({k: len(v) for k, v in all_channel_messages.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPACE_REG = re.compile(r'\\s+')\n",
    "encoding = tiktoken.encoding_for_model(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "\n",
    "def format_message(message: Message) -> str | None:\n",
    "    username = message.author.username\n",
    "    author = f'{gn} ({username})' if (gn := message.author.global_name) else username\n",
    "    content = SPACE_REG.sub(' ', message.content).strip()\n",
    "    if content:\n",
    "        return (\n",
    "            f'Author: {author}; '\n",
    "            f'Timestamp: {message.timestamp:%Y-%m-%d %H:%M:%S}; '\n",
    "            f'Content: {content}\\n'\n",
    "        )\n",
    "\n",
    "\n",
    "def yield_pages(messages: Iterable[Message]):\n",
    "    page_messages: list[Message] = []\n",
    "    page_lines: list[str] = []\n",
    "    page_tokens = 0\n",
    "    for message in messages:\n",
    "        line = format_message(message)\n",
    "        if not line:\n",
    "            continue\n",
    "        line_tokens = len(encoding.encode(line))\n",
    "        if line_tokens > EMBEDDING_MODEL_MAX_TOKENS:\n",
    "            if page_lines:\n",
    "                yield page_messages, ''.join(page_lines)\n",
    "                page_messages, page_lines, page_tokens = overlap(page_messages, page_lines)\n",
    "            yield [message], line\n",
    "            continue\n",
    "        if len(page_messages) == 100 or page_tokens + line_tokens > EMBEDDING_MODEL_MAX_TOKENS:\n",
    "            yield page_messages, ''.join(page_lines)\n",
    "            page_messages, page_lines, page_tokens = overlap(page_messages, page_lines)\n",
    "        page_messages.append(message)\n",
    "        page_lines.append(line)\n",
    "        page_tokens += line_tokens\n",
    "    if page_lines:\n",
    "        yield page_messages, ''.join(page_lines)\n",
    "\n",
    "\n",
    "def overlap(\n",
    "    lines_messages: list[Message], lines: list[str]\n",
    ") -> tuple[list[Message], list[str], int]:\n",
    "    assert len(lines_messages) == len(lines)\n",
    "    messages_overlap = lines_messages[int(len(lines_messages) * 0.8) :]\n",
    "    lines_overlap = lines[int(len(lines) * 0.8) :]\n",
    "    return messages_overlap, lines_overlap, len(encoding.encode(''.join(lines_overlap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages: list[MessagePage] = []\n",
    "for channel_id, channel_messages in all_channel_messages.items():\n",
    "    channel_messages.sort(key=lambda m: m.timestamp)\n",
    "    for messages, context in yield_pages(channel_messages):\n",
    "        pages.append(MessagePage(context=context, channel_id=channel_id, messages=messages))\n",
    "\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed and store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGE_INSERT_QUERY = r\"\"\"\n",
    "with\n",
    "    context := <str>$context,\n",
    "    channel_id := <str>$channel_id,\n",
    "    message_ids := <array<str>>$message_ids,\n",
    "    messages := (\n",
    "        select discord::Message\n",
    "        filter .client = global client and .message_id in array_unpack(message_ids)\n",
    "    )\n",
    "insert discord::MessagePage {\n",
    "    client := global client,\n",
    "    context := context,\n",
    "    channel_id := channel_id,\n",
    "    messages := messages,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "for page in pages:\n",
    "    await client.query(\n",
    "        PAGE_INSERT_QUERY,\n",
    "        context=page.context,\n",
    "        channel_id=page.channel_id,\n",
    "        message_ids=[m.id for m in page.messages],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve & Generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = await gel.ai.create_async_rag_client(client, model='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"\"\"\n",
    "Que pense bidon de la censure ?\n",
    "\"\"\"\n",
    "\n",
    "RAG_QUERY = r\"\"\"\n",
    "with\n",
    "    embeddings := <array<float32>>$embeddings\n",
    "select ext::ai::search(discord::MessagePage { * }, embeddings)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    object: MessagePage\n",
    "    distance: float\n",
    "\n",
    "\n",
    "search_adapter = TypeAdapter(list[SearchResult])\n",
    "\n",
    "model = PYDANTIC_AI_MODEL_CLS(\n",
    "    PYDANTIC_AI_DEFAULT_MODEL_NAME,\n",
    "    provider=PYDANTIC_AI_PROVIDER,\n",
    ")\n",
    "\n",
    "agent = Agent(model, system_prompt='The assistant should retrieve context before answering.')\n",
    "\n",
    "\n",
    "@agent.tool_plain\n",
    "async def retrieve(search_query: str) -> str:\n",
    "    \"\"\"Retrieve chat sections based on a search query in French.\"\"\"\n",
    "    print(search_query)\n",
    "    embeddings = await rag.generate_embeddings(search_query, model=EMBEDDING_MODEL_NAME)\n",
    "    resp = await client.query_json(RAG_QUERY, embeddings=embeddings[:2000])\n",
    "    results = search_adapter.validate_json(resp)\n",
    "    pages = [p.object.context for p in results[:50]]\n",
    "    context = '\\n-------------------------\\n'.join(pages)\n",
    "    return context\n",
    "\n",
    "\n",
    "async with agent.iter(QUESTION) as run:\n",
    "    async for node in run:\n",
    "        if Agent.is_user_prompt_node(node):\n",
    "            # A user prompt node => The user has provided input\n",
    "            ...\n",
    "        elif Agent.is_model_request_node(node):\n",
    "            # A model request node => We can stream tokens from the model's request\n",
    "            async with node.stream(run.ctx) as request_stream:\n",
    "                async for event in request_stream:\n",
    "                    if isinstance(event, PartStartEvent):\n",
    "                        if isinstance(event.part, TextPart):\n",
    "                            print(event.part.content, end='')\n",
    "                    elif isinstance(event, PartDeltaEvent):\n",
    "                        if isinstance(event.delta, TextPartDelta):\n",
    "                            print(event.delta.content_delta, end='')\n",
    "                        elif isinstance(event.delta, ToolCallPartDelta):\n",
    "                            ...\n",
    "                    elif isinstance(event, FinalResultEvent):\n",
    "                        ...\n",
    "                print()\n",
    "        elif Agent.is_call_tools_node(node):\n",
    "            # A handle-response node => The model returned some data, potentially calls a tool\n",
    "            async with node.stream(run.ctx) as handle_stream:\n",
    "                async for event in handle_stream:\n",
    "                    if isinstance(event, FunctionToolCallEvent):\n",
    "                        ...\n",
    "                    elif isinstance(event, FunctionToolResultEvent):\n",
    "                        ...\n",
    "        elif Agent.is_end_node(node):\n",
    "            assert run.result and run.result.output == node.data.output\n",
    "            # Once an End node is reached, the agent run is complete\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
